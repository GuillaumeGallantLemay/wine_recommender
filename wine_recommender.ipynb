{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook consists of a few different sections, each supporting a different step in the creation of a serverless wine recommender API:\n",
    "\n",
    "1. Preprocess wine review dataset\n",
    "2. Train wine word embeddings using BlazingText model\n",
    "3. Generate lookup table with wine word embeddings from trained Blazingtext model\n",
    "4. Convert wine reviews to wine embeddings\n",
    "5. Train Nearest Neighbors model on wine embeddings\n",
    "\n",
    "First, we need to install the necessary libraries. We also need to run a pip install of the gensim library to access it from within Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/4b/19eecdf07d614665fa889857dc56ac965631c7bd816c3476d2f0cac6ea3b/gensim-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 2.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 43.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.20.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (1.9.178)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.23)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.178 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.178)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.178->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.178->boto3->smart-open>=1.7.0->gensim) (2.7.3)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.7.3 smart-open-1.8.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess the wine review dataset\n",
    "\n",
    "To preprocess the text in the wine reviews, we firs tneed to load the full dataset from the S3 bucket in which it has been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tandem 2011 Ars In Vitro Tempranillo-Merlot (N...</td>\n",
       "      <td>Blackberry and raspberry aromas show a typical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Terre di Giurfo 2013 Belsito Frappato (Vittoria)</td>\n",
       "      <td>Here's a bright, informal red that opens with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trimbach 2012 Gewurztraminer (Alsace)</td>\n",
       "      <td>This dry and restrained wine offers spice in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heinz Eifel 2013 Shine Gewürztraminer (Rheinhe...</td>\n",
       "      <td>Savory dried thyme notes accent sunnier flavor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jean-Baptiste Adam 2012 Les Natures Pinot Gris...</td>\n",
       "      <td>This has great depth of flavor with its fresh ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  \\\n",
       "0                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2      Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4  Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "5  Tandem 2011 Ars In Vitro Tempranillo-Merlot (N...   \n",
       "6   Terre di Giurfo 2013 Belsito Frappato (Vittoria)   \n",
       "7              Trimbach 2012 Gewurztraminer (Alsace)   \n",
       "8  Heinz Eifel 2013 Shine Gewürztraminer (Rheinhe...   \n",
       "9  Jean-Baptiste Adam 2012 Les Natures Pinot Gris...   \n",
       "\n",
       "                                         Description  \n",
       "0  Aromas include tropical fruit, broom, brimston...  \n",
       "1  This is ripe and fruity, a wine that is smooth...  \n",
       "2  Tart and snappy, the flavors of lime flesh and...  \n",
       "3  Pineapple rind, lemon pith and orange blossom ...  \n",
       "4  Much like the regular bottling from 2012, this...  \n",
       "5  Blackberry and raspberry aromas show a typical...  \n",
       "6  Here's a bright, informal red that opens with ...  \n",
       "7  This dry and restrained wine offers spice in p...  \n",
       "8  Savory dried thyme notes accent sunnier flavor...  \n",
       "9  This has great depth of flavor with its fresh ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "bucket='data-science-wine-reviews'\n",
    "data_key = 'full_wine_dataset.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "wine_dataset = pd.read_csv(data_location, low_memory=False)\n",
    "wine_dataset_relevant = wine_dataset[['Name', 'Description']]\n",
    "wine_dataset_relevant.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build vector representations of our wine reviews, we first need to train word embeddings on the various wine terms in our corpus. This involved a couple of steps:\n",
    "\n",
    "- concatenate all the wine reviews in the corpus\n",
    "- tokenize into sentences\n",
    "- remove stopwords\n",
    "- convert to lower case\n",
    "- remove special characters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = list(wine_dataset_relevant['Description'])\n",
    "reviews_list = [str(r) for r in reviews_list]\n",
    "full_corpus = ' '.join(reviews_list)\n",
    "sentences_tokenized = sent_tokenize(full_corpus)\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "punctuation_table = str.maketrans({key: None for key in string.punctuation})\n",
    "sno = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(raw_text):\n",
    "    try:\n",
    "        word_list = word_tokenize(raw_text)\n",
    "        normalized_sentence = []\n",
    "        for w in word_list:\n",
    "            try:\n",
    "                w = str(w)\n",
    "                lower_case_word = str.lower(w)\n",
    "                stemmed_word = sno.stem(lower_case_word)\n",
    "                no_punctuation = stemmed_word.translate(punctuation_table)\n",
    "                if len(no_punctuation) > 1 and no_punctuation not in stop_words:\n",
    "                    normalized_sentence.append(no_punctuation)\n",
    "            except:\n",
    "                continue\n",
    "        return normalized_sentence\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_sample = sentences_tokenized[:10]\n",
    "normalized_sentences = []\n",
    "for s in sentences_tokenized:\n",
    "    normalized_text = normalize_text(s)\n",
    "    normalized_sentences.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the gensim Phrases package to extract bi- and tri-grams from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(normalized_sentences)\n",
    "phrases = Phrases(phrases[normalized_sentences])\n",
    "\n",
    "ngrams = Phraser(phrases)\n",
    "\n",
    "phrased_sentences = []\n",
    "for sent in normalized_sentences:\n",
    "    phrased_sentence = ngrams[sent]\n",
    "    phrased_sentences.append(phrased_sentence)\n",
    "\n",
    "full_list_words = [item for sublist in phrased_sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous chapter of this work, we have mapped commonly appearing and semantically meaningful words, bi-grams and tri-grams from wine reviews to a standardized set of wine descriptors. We will now apply this mapping to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_mapping = pd.read_csv('s3://{}/descriptor_mapping.csv'.format(bucket)).set_index('raw descriptor')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "def return_mapped_descriptor(word):\n",
    "    if word in list(descriptor_mapping.index):\n",
    "        normalized_word = descriptor_mapping['level_3'][word]\n",
    "        return normalized_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "normalized_sentences = []\n",
    "for sent in phrased_sentences:\n",
    "    normalized_sentence = []\n",
    "    for word in sent:\n",
    "        normalized_word = return_mapped_descriptor(word)\n",
    "        normalized_sentence.append(str(normalized_word))\n",
    "    normalized_sentence.append('.')\n",
    "    normalized_sentence_concat = ' '.join(normalized_sentence)\n",
    "    normalized_sentences.append(normalized_sentence_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the BlazingText algorithm in Sagemaker, the training data needs to be stored in a .txt file. We will write our normalized corpus to a .txt file and save this in our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wine_corpus.txt', 'w') as f:\n",
    "    for item in normalized_sentences:\n",
    "        f.write(\"{}\\n\".format(item))\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('wine-corpus.txt').upload_file('wine_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train wine word embeddings using BlazingText model\n",
    "\n",
    "Now that the training data has been prepared, we can turn our attention to training the BlazingText model. We need to define a location for the training data and an output location for the model. We also need to define a container for the Blazingtext algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest (us-east-2)\n"
     ]
    }
   ],
   "source": [
    "train_data = 's3://{}/wine-corpus.txt'.format(bucket)\n",
    "s3_output_location = 's3://{}/output'.format(bucket)\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set the specifications of the instance that we will use to train the Blazingtext model, and choose the hyperparameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=2, \n",
    "                                         train_instance_type='ml.c4.2xlarge',\n",
    "                                         train_volume_size = 5,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "bt_model.set_hyperparameters(mode=\"batch_skipgram\",\n",
    "                             epochs=15,\n",
    "                             min_count=5,\n",
    "                             sampling_threshold=0.0001,\n",
    "                             learning_rate=0.05,\n",
    "                             window_size=5,\n",
    "                             vector_dim=300,\n",
    "                             negative_samples=5,\n",
    "                             batch_size=11, #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "                             evaluation=True,# Perform similarity evaluation on WS-353 dataset at the end of training\n",
    "                             subwords=False) # Subword embedding learning is not supported by batch_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed the training data to the model, we need to set a channel for it to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-05 16:25:02 Starting - Starting the training job...\n",
      "2019-07-05 16:25:03 Starting - Launching requested ML instances......\n",
      "2019-07-05 16:26:05 Starting - Preparing the instances for training...\n",
      "2019-07-05 16:26:59 Downloading - Downloading input data\n",
      "2019-07-05 16:26:59 Training - Downloading the training image..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31mFound 10.0.241.111 for host algo-1\u001b[0m\n",
      "\u001b[31mFound 10.0.202.136 for host algo-2\u001b[0m\n",
      "\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[32mFound 10.0.241.111 for host algo-1\u001b[0m\n",
      "\u001b[32mFound 10.0.202.136 for host algo-2\u001b[0m\n",
      "\n",
      "2019-07-05 16:27:12 Training - Training image download completed. Training in progress.\u001b[32m[07/05/2019 16:27:24 WARNING 140673357956928] Loggers have already been setup.\u001b[0m\n",
      "\u001b[32m[07/05/2019 16:27:24 WARNING 140673357956928] Loggers have already been setup.\u001b[0m\n",
      "\u001b[32m[07/05/2019 16:27:24 INFO 140673357956928] nvidia-smi took: 0.0251688957214 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[07/05/2019 16:27:24 INFO 140673357956928] Running distributed CPU BlazingText training using batch_skipgram on 2 hosts.\u001b[0m\n",
      "\u001b[32m[07/05/2019 16:27:24 INFO 140673357956928] Number of hosts: 2, master IP address: 10.0.241.111, host IP address: 10.0.202.136.\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 WARNING 139910265243456] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 WARNING 139910265243456] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 INFO 139910265243456] nvidia-smi took: 0.0252530574799 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 INFO 139910265243456] Running distributed CPU BlazingText training using batch_skipgram on 2 hosts.\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 INFO 139910265243456] Number of hosts: 2, master IP address: 10.0.241.111, host IP address: 10.0.241.111.\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 INFO 139910265243456] HTTP server started....\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:24 INFO 139910265243456] Processing /opt/ml/input/data/train/wine-corpus.txt . File size: 20 MB\u001b[0m\n",
      "\u001b[31mWarning: Permanently added 'algo-2,10.0.202.136' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[31mprocessor name: algo-1, number of processors: 2, rank: 0\u001b[0m\n",
      "\u001b[31mprocessor name: algo-2, number of processors: 2, rank: 1\u001b[0m\n",
      "\u001b[31mRead 3M words\u001b[0m\n",
      "\u001b[31mNumber of words:  16135\u001b[0m\n",
      "\u001b[31mAlpha: 0.0488  Progress: 2.48%  Million Words/sec: 3.17\u001b[0m\n",
      "\u001b[31mAlpha: 0.0463  Progress: 7.52%  Million Words/sec: 3.50\u001b[0m\n",
      "\u001b[31mAlpha: 0.0435  Progress: 13.19%  Million Words/sec: 3.49\u001b[0m\n",
      "\u001b[31mAlpha: 0.0406  Progress: 18.92%  Million Words/sec: 3.53\u001b[0m\n",
      "\u001b[31mAlpha: 0.0381  Progress: 23.95%  Million Words/sec: 3.53\u001b[0m\n",
      "\u001b[31mAlpha: 0.0353  Progress: 29.64%  Million Words/sec: 3.55\u001b[0m\n",
      "\u001b[31mAlpha: 0.0328  Progress: 34.68%  Million Words/sec: 3.55\u001b[0m\n",
      "\u001b[31mAlpha: 0.0303  Progress: 39.73%  Million Words/sec: 3.56\u001b[0m\n",
      "\u001b[31mAlpha: 0.0275  Progress: 45.44%  Million Words/sec: 3.55\u001b[0m\n",
      "\u001b[31mAlpha: 0.0250  Progress: 50.47%  Million Words/sec: 3.57\u001b[0m\n",
      "\u001b[31mAlpha: 0.0221  Progress: 56.21%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0192  Progress: 61.91%  Million Words/sec: 3.57\u001b[0m\n",
      "\u001b[31mAlpha: 0.0167  Progress: 66.94%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0142  Progress: 71.96%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0117  Progress: 77.03%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0089  Progress: 82.71%  Million Words/sec: 3.57\u001b[0m\n",
      "\u001b[31mAlpha: 0.0060  Progress: 88.41%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0035  Progress: 93.49%  Million Words/sec: 3.58\u001b[0m\n",
      "\u001b[31mAlpha: 0.0008  Progress: 98.85%  Million Words/sec: 3.57\u001b[0m\n",
      "\u001b[31mAlpha: 0.0000  Progress: 100.00%  Million Words/sec: 3.46\n",
      "\u001b[0m\n",
      "\u001b[31mTraining finished!\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 3.46\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 16.00\u001b[0m\n",
      "\u001b[31mEvaluating word embeddings....\u001b[0m\n",
      "\u001b[31mVectors read from: /opt/ml/model/vectors.txt \u001b[0m\n",
      "\u001b[31m{\n",
      "    \"EN-WS-353-ALL.txt\": {\n",
      "        \"not_found\": 275, \n",
      "        \"spearmans_rho\": 0.45202655764440625, \n",
      "        \"total_pairs\": 353\n",
      "    }, \n",
      "    \"EN-WS-353-REL.txt\": {\n",
      "        \"not_found\": 202, \n",
      "        \"spearmans_rho\": 0.32392066614377446, \n",
      "        \"total_pairs\": 252\n",
      "    }, \n",
      "    \"EN-WS-353-SIM.txt\": {\n",
      "        \"not_found\": 153, \n",
      "        \"spearmans_rho\": 0.5357057125031585, \n",
      "        \"total_pairs\": 203\n",
      "    }, \n",
      "    \"mean_rho\": 0.4372176454304464\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m[07/05/2019 16:27:52 INFO 139910265243456] #mean_rho: 0.4372176454304464\u001b[0m\n",
      "\u001b[32m[07/05/2019 16:29:02 INFO 140673357956928] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\n",
      "2019-07-05 16:29:21 Uploading - Uploading generated training model\n",
      "2019-07-05 16:29:21 Completed - Training job completed\n",
      "Billable seconds: 324\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure an endpoint to host the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate lookup table with wine word embeddings from trained Blazingtext model\n",
    "\n",
    "We do not need to host an endpoint for our model. Rather, we want to use our model to produce a lookup table for all the wine-related terms in our corpus. We can then use a lambda function to make retrieve information from this lookup table. This will be cheaper and more efficient than permanently hosting an endpoint for our BlazingText model.\n",
    "\n",
    "To create this lookup table, we will first download the model tarfile from the S3 bucket it was saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "key = bt_model.model_data[bt_model.model_data.find(\"/\", 5)+1:]\n",
    "s3.Bucket(bucket).download_file(key, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then open the tarfile - we see that it consists of three items. We are most interested in the vectors.txt file, which is a text file with our trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors.bin\n",
      "vectors.txt\n",
      "eval.json\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can open the vectors.txt file. We will only keep those descriptors that are in our 'descriptor mapping' of common and meaningful wine descriptors.\n",
    "\n",
    "We will save the resulting csv file in our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "num_points = len(open('vectors.txt','r').read().split('\\n'))\n",
    "\n",
    "first_line = True\n",
    "index_to_word = []\n",
    "with open(\"vectors.txt\",\"r\") as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if first_line:\n",
    "            dim = int(line.strip().split()[1])\n",
    "            word_vecs = np.zeros((num_points, dim), dtype=float)\n",
    "            first_line = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        vec = word_vecs[line_num-1]\n",
    "        for index, vec_val in enumerate(line.split()[1:]):\n",
    "            vec[index] = float(vec_val)\n",
    "        index_to_word.append(word)\n",
    "        if line_num >= num_points:\n",
    "            break\n",
    "word_vecs = normalize(word_vecs, copy=False, return_norm=False)\n",
    "\n",
    "names_vecs = list(zip(index_to_word, word_vecs))\n",
    "\n",
    "names_vecs_filtered = [n for n in names_vecs if n[0] in list(descriptor_mapping['level_3'])]\n",
    "\n",
    "names_vecs_df = pd.DataFrame(names_vecs_filtered, columns=['word', 'vector'])\n",
    "names_vecs_df.sort_values(by=['word'], inplace=True)\n",
    "names_vecs_df.to_csv('word_vectors.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('word_vectors.csv').upload_file('word_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert wine reviews to wine embeddings\n",
    "\n",
    "Now that we have our word embeddings, we can turn our attention to creating 'wine embeddings': a single vector representation of each wine review. We will go through a few steps to achieve this:\n",
    "\n",
    "1. Retrieve descriptors from each wine review\n",
    "2. Use mapping of wine descriptors to 'standardize' these terms\n",
    "3. Retrieve the word vectors for these standardized wine descriptors\n",
    "4. Weight each word vector in the wine review by a TF-IDF weighting\n",
    "5. Take the sum of all the word vectors in each wine review to produce a single 'wine embedding'\n",
    "\n",
    "First, let's retrieve the descriptors from each wine review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = list(wine_dataset_relevant['Description'])\n",
    "\n",
    "def return_descriptor_from_mapping(word):\n",
    "    if word in list(descriptor_mapping.index):\n",
    "        descriptor_to_return = descriptor_mapping['level_3'][word]\n",
    "        return descriptor_to_return\n",
    "\n",
    "descriptorized_reviews = []\n",
    "for review in wine_reviews:\n",
    "    normalized_review = normalize_text(review)\n",
    "    phrased_review = ngrams[normalized_review]\n",
    "    descriptors_only = [return_descriptor_from_mapping(word) for word in phrased_review]\n",
    "    no_nones = [str(d) for d in descriptors_only if d is not None]\n",
    "    descriptorized_review = ' '.join(no_nones)\n",
    "    descriptorized_reviews.append(descriptorized_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having a separate file with the IDF scores for each word and a separate file with all the word vectors, we will create a single consolidated file with the IDF-weighted word vectors. This will be more efficient later on in our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit(descriptorized_reviews)\n",
    "\n",
    "dict_of_idf_weightings = pd.DataFrame(zip(X.get_feature_names(), X.idf_), columns=['word', 'idf'])\n",
    "\n",
    "vectors_and_idf = pd.merge(left=names_vecs_df, right=dict_of_idf_weightings, left_on='word', right_on='word', how='inner')\n",
    "vectors_and_idf['word_vec_idf'] = vectors_and_idf['vector']*vectors_and_idf['idf']\n",
    "vectors_and_idf = vectors_and_idf[['word', 'word_vec_idf']]\n",
    "vectors_and_idf.set_index('word', inplace=True)\n",
    "vectors_and_idf.to_csv('word_vectors_idf.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('word_vectors_idf.csv').upload_file('word_vectors_idf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all the necessary individual pieces to create a precalculated embedding for every wine review. This will be the input variable for our nearest neighbors recommender model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>descriptors</th>\n",
       "      <th>review_vector</th>\n",
       "      <th>descriptor_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>[tropical_fruit, fruit, dry, herb, apple, citr...</td>\n",
       "      <td>[-0.032441616981936545, 0.15638791585721334, -...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>[ripe, fruit, smooth, firm, juicy, berry, frui...</td>\n",
       "      <td>[-0.08550763328767134, 0.09603241912986349, 0....</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>[tart, snappy, lime, green, pineapple, crisp, ...</td>\n",
       "      <td>[0.055965694257395136, 0.1996108806031312, -0....</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>[pineapple, rind, lemon_pith, orange_blossom, ...</td>\n",
       "      <td>[0.11896378047799183, 0.12080962220033609, -0....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>[rough, tannin, rustic, earth, herb]</td>\n",
       "      <td>[-0.12856429135555994, 0.12319225682620301, 0....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  \\\n",
       "0                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2      Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4  Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Aromas include tropical fruit, broom, brimston...   \n",
       "1  This is ripe and fruity, a wine that is smooth...   \n",
       "2  Tart and snappy, the flavors of lime flesh and...   \n",
       "3  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                                         descriptors  \\\n",
       "0  [tropical_fruit, fruit, dry, herb, apple, citr...   \n",
       "1  [ripe, fruit, smooth, firm, juicy, berry, frui...   \n",
       "2  [tart, snappy, lime, green, pineapple, crisp, ...   \n",
       "3  [pineapple, rind, lemon_pith, orange_blossom, ...   \n",
       "4               [rough, tannin, rustic, earth, herb]   \n",
       "\n",
       "                                       review_vector  descriptor_count  \n",
       "0  [-0.032441616981936545, 0.15638791585721334, -...                 9  \n",
       "1  [-0.08550763328767134, 0.09603241912986349, 0....                 8  \n",
       "2  [0.055965694257395136, 0.1996108806031312, -0....                 7  \n",
       "3  [0.11896378047799183, 0.12080962220033609, -0....                 6  \n",
       "4  [-0.12856429135555994, 0.12319225682620301, 0....                 5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_review_vectors = []\n",
    "for d in descriptorized_reviews:\n",
    "    descriptor_count = 0\n",
    "    weighted_review_terms = []\n",
    "    terms = d.split(' ')\n",
    "    \n",
    "    for term in terms:\n",
    "        if term in list(vectors_and_idf.index):\n",
    "            weighted_word_vector = vectors_and_idf.at[term, 'word_vec_idf']\n",
    "            weighted_review_terms.append(weighted_word_vector)\n",
    "            descriptor_count += 1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        review_vector = sum(weighted_review_terms)/len(weighted_review_terms)\n",
    "    except:\n",
    "        review_vector = []\n",
    "    \n",
    "    vector_and_count = [terms, review_vector, descriptor_count]\n",
    "    wine_review_vectors.append(vector_and_count)\n",
    "\n",
    "wine_review_vectors_df = pd.DataFrame(wine_review_vectors, columns=['descriptors', 'review_vector', 'descriptor_count'])\n",
    "full_wine_df = pd.concat([wine_dataset_relevant, wine_review_vectors_df], axis=1)\n",
    "full_wine_df.dropna(how='any', inplace=True)\n",
    "full_wine_df.drop_duplicates(subset=['Name'], inplace=True)\n",
    "full_wine_df.to_csv('wine_review_vectors.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('wine_review_vectors.csv').upload_file('wine_review_vectors.csv')\n",
    "full_wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Just some final preparation before we train the model. As a part of this, we will remove any wine reviews with fewer than 5 descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_vectors = pd.read_csv('s3://data-science-wine-reviews/nearest_neighbors/data/wine_review_vectors.csv').set_index('Name')\n",
    "\n",
    "def convert_to_list(raw_review_vec):\n",
    "    review_vec_trimmed = raw_review_vec.replace('[', '').replace(']', '')\n",
    "    review_vec = np.fromstring(review_vec_trimmed, dtype=float, sep='  ')\n",
    "    review_vec_list = review_vec.tolist()\n",
    "    return review_vec_list\n",
    "    \n",
    "wine_vectors['review_vec'] = wine_vectors['review_vector'].apply(convert_to_list)\n",
    "\n",
    "def count_dim(review_vec):\n",
    "    vec_dim = len(review_vec)\n",
    "    return vec_dim\n",
    "\n",
    "wine_vectors['vec_dim'] = wine_vectors['review_vec'].apply(count_dim)\n",
    "wine_vectors_filtered = wine_vectors.loc[wine_vectors['vec_dim']==300]\n",
    "wine_vectors_filtered = wine_vectors.loc[wine_vectors['descriptor_count']>=5]\n",
    "wine_vectors_filtered = wine_vectors_filtered[['Description', 'descriptors', 'review_vector', 'descriptor_count']]\n",
    "\n",
    "wine_vectors_filtered.to_csv('wine_review_vectors.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('nearest_neighbors/data/wine_review_vectors.csv').upload_file('wine_review_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Nearest Neighbors model on wine embeddings\n",
    "\n",
    "Now, we have everything we need to train our Nearest Neighbors model. Since we will be using an SKLearn implementation of this model, we have to make use of the 'model serving' functionality in the SageMaker Python SDK. This functionality allows us to write custom functions to ingest the data, make predictions (in our case, return the X Nearest Neighbors for a given wine embedding) and return the output in the format we desire. These functions sit in the sklearn_nearest_neighbors.py file. We can call this file, configure our training instance and specify the hyperparameters of our Nearest Neighbors model below/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'sklearn_nearest_neighbors.py'\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={'n_neighbors': 10, 'metric': 'cosine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-05 16:41:50 Starting - Starting the training job...\n",
      "2019-07-05 16:41:52 Starting - Launching requested ML instances......\n",
      "2019-07-05 16:42:58 Starting - Preparing the instances for training...\n",
      "2019-07-05 16:43:48 Downloading - Downloading input data...\n",
      "2019-07-05 16:44:19 Training - Training image download completed. Training in progress..\n",
      "\u001b[31m2019-07-05 16:44:19,468 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,470 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,482 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,818 sagemaker-containers INFO     Module sklearn_nearest_neighbors does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,818 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,819 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:19,819 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: sklearn-nearest-neighbors\n",
      "  Building wheel for sklearn-nearest-neighbors (setup.py): started\n",
      "  Building wheel for sklearn-nearest-neighbors (setup.py): finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gol6vyn8/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built sklearn-nearest-neighbors\u001b[0m\n",
      "\u001b[31mInstalling collected packages: sklearn-nearest-neighbors\u001b[0m\n",
      "\u001b[31mSuccessfully installed sklearn-nearest-neighbors-1.0.0\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:20,943 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:20,954 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"is_master\": true,\n",
      "    \"num_gpus\": 0,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"user_entry_point\": \"sklearn_nearest_neighbors.py\",\n",
      "    \"hyperparameters\": {\n",
      "        \"metric\": \"cosine\",\n",
      "        \"n_neighbors\": 10\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-820407560908/sagemaker-scikit-learn-2019-07-05-16-41-49-894/source/sourcedir.tar.gz\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2019-07-05-16-41-49-894\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"module_name\": \"sklearn_nearest_neighbors\",\n",
      "    \"log_level\": 20\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=sklearn_nearest_neighbors.py\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=sklearn_nearest_neighbors\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--metric\",\"cosine\",\"--n_neighbors\",\"10\"]\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_HP_N_NEIGHBORS=10\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"metric\":\"cosine\",\"n_neighbors\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2019-07-05-16-41-49-894\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-820407560908/sagemaker-scikit-learn-2019-07-05-16-41-49-894/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_nearest_neighbors\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_nearest_neighbors.py\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_HPS={\"metric\":\"cosine\",\"n_neighbors\":10}\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-820407560908/sagemaker-scikit-learn-2019-07-05-16-41-49-894/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_HP_METRIC=cosine\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=2\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m sklearn_nearest_neighbors --metric cosine --n_neighbors 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mCollecting s3fs\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/fa/10093c48f0def7878f8945429c699e36d73e49b90a282ace2b7a553ed020/s3fs-0.2.2.tar.gz (47kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: boto3>=1.9.91 in /usr/local/lib/python3.5/dist-packages (from s3fs) (1.9.159)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: botocore>=1.12.91 in /usr/local/lib/python3.5/dist-packages (from s3fs) (1.12.159)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from s3fs) (1.12.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.5/dist-packages (from boto3>=1.9.91->s3fs) (0.2.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.5/dist-packages (from boto3>=1.9.91->s3fs) (0.9.4)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.5/dist-packages (from botocore>=1.12.91->s3fs) (0.14)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.5/dist-packages (from botocore>=1.12.91->s3fs) (2.8.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.5/dist-packages (from botocore>=1.12.91->s3fs) (1.25.3)\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: s3fs\n",
      "  Building wheel for s3fs (setup.py): started\n",
      "  Building wheel for s3fs (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/20/c9/b0256fe423de22bca83518fe0f7eba34ae1b9591a9d8c2d3b4\u001b[0m\n",
      "\u001b[31mSuccessfully built s3fs\u001b[0m\n",
      "\u001b[31mInstalling collected packages: s3fs\u001b[0m\n",
      "\u001b[31mSuccessfully installed s3fs-0.2.2\u001b[0m\n",
      "\n",
      "2019-07-05 16:44:50 Uploading - Uploading generated training model\u001b[31mdata has been transformed\u001b[0m\n",
      "\u001b[31mmodel has been fitted\u001b[0m\n",
      "\u001b[31m2019-07-05 16:44:48,612 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-07-05 16:45:26 Completed - Training job completed\n",
      "Billable seconds: 98\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({'train': 's3://data-science-wine-reviews/nearest_neighbors/data/wine_review_vectors.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model, we can deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run a quick test to make sure that the model endpoint is returning the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_vectors = pd.read_csv('s3://data-science-wine-reviews/nearest_neighbors/data/wine_review_vectors_sample.csv')\n",
    "\n",
    "def convert_to_list(raw_review_vec):\n",
    "    review_vec_trimmed = raw_review_vec.replace('[', '').replace(']', '')\n",
    "    review_vec = np.fromstring(review_vec_trimmed, dtype=float, sep='  ')\n",
    "    review_vec_list = review_vec.tolist()\n",
    "    return review_vec_list\n",
    "    \n",
    "wine_vectors['review_vec'] = wine_vectors['review_vector'].apply(convert_to_list)\n",
    "\n",
    "sample_vector = wine_vectors.at[97, 'review_vec']\n",
    "sample_vector = np.asarray(sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = predictor.predict(sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.37459606e-01 1.42040288e-01 1.46988100e-01 1.54312524e-01\n",
      "  1.56549391e-01 1.62581288e-01 1.62581288e-01 1.62931791e-01\n",
      "  1.63314825e-01 1.65550581e-01]\n",
      " [9.19130000e+04 2.49230000e+04 7.40960000e+04 2.64920000e+04\n",
      "  7.71960000e+04 9.68710000e+04 1.13695000e+05 8.74650000e+04\n",
      "  1.00823000e+05 1.44780000e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
